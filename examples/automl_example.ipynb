{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea91d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c815af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load the dataset\n",
    "# Each row contains: \n",
    "#   - igef: unique vehicle identifier\n",
    "#   - test: name of the performed test (e.g. \"test_17\", \"test_80\", etc.)\n",
    "#   - test_result: categorical outcome (\"OK\" or \"NOK\")\n",
    "conn = sqlite3.connect(\"dummy_data.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM dummy_data\", conn)\n",
    "conn.close()\n",
    "\n",
    "# 2) Define the test to predict and the test split ratio\n",
    "# The goal is to predict the result of \"test_80\" based on all other tests\n",
    "label = \"test_80\"\n",
    "test_size = 0.2\n",
    "\n",
    "# 3) Convert test results to binary form\n",
    "# \"OK\" becomes 0, \"NOK\" becomes 1\n",
    "# This makes aggregation and modeling easier\n",
    "df[\"binary_test_result\"] = df[\"test_result\"].map({\"OK\": 0, \"NOK\": 1})\n",
    "\n",
    "# 4) Extract target labels\n",
    "# For each vehicle (igef), determine the outcome of test_80\n",
    "# If a vehicle has multiple entries for test_80, take the maximum\n",
    "# (since NOK=1 overrides OK=0)\n",
    "y = (\n",
    "    df[df[\"test\"] == label]\n",
    "    .groupby(\"igef\")[\"binary_test_result\"]\n",
    "    .max()\n",
    "    .rename(label)\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "# 5) Build the feature matrix\n",
    "# Pivot the table so that:\n",
    "#   - each row = one vehicle\n",
    "#   - each column = one test (excluding test_80)\n",
    "#   - cell value = binary test result (0 or 1)\n",
    "# Missing values are filled with 0, assuming the test was not failed\n",
    "X = (\n",
    "    df[df[\"test\"] != label]\n",
    "    .pivot_table(index=\"igef\", columns=\"test\", values=\"binary_test_result\", aggfunc=\"max\")\n",
    "    .sort_index(axis=1)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# 6) Combine features and target labels\n",
    "# Keep only vehicles with a known test_80 result\n",
    "data = X.join(y, how=\"inner\").reset_index()\n",
    "\n",
    "# 7) Split into training and test sets by vehicle\n",
    "# Stratify by the label to maintain the same OK/NOK ratio in both sets\n",
    "# Splitting by vehicle prevents data leakage (a vehicle appears only once)\n",
    "igefs = data[\"igef\"].unique()\n",
    "igef_train, igef_test = train_test_split(\n",
    "    igefs,\n",
    "    test_size=test_size,\n",
    "    stratify=data[label],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train = data[data[\"igef\"].isin(igef_train)]\n",
    "test = data[data[\"igef\"].isin(igef_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d8b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251030_085810\"\n",
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:34 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8103\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.35 GB / 16.00 GB (33.4%)\n",
      "Disk Space Avail:   65.62 GB / 228.27 GB (28.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/Users/lpossner/Projects/databot/AutogluonModels/ag-20251030_085810\"\n",
      "Train Data Rows:    800\n",
      "Train Data Columns: 80\n",
      "Label Column:       test_80\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [np.int64(0), np.int64(1)]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5476.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 80 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 80 | ['test_0', 'test_1', 'test_10', 'test_11', 'test_12', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['bool']) : 80 | ['test_0', 'test_1', 'test_10', 'test_11', 'test_12', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1199.47s of the 1799.65s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.34%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1194.40s of the 1794.58s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.38%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1191.66s of the 1791.84s of remaining time.\n",
      "\t0.9231\t = Validation score   (f1)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1191.07s of the 1791.25s of remaining time.\n",
      "\t0.963\t = Validation score   (f1)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1190.77s of the 1790.95s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=8.67%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t6.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1182.56s of the 1782.74s of remaining time.\n",
      "\t0.963\t = Validation score   (f1)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1182.20s of the 1782.38s of remaining time.\n",
      "\t0.9756\t = Validation score   (f1)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1181.87s of the 1782.05s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t2.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1178.03s of the 1778.22s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.78%)\n",
      "\t0.6452\t = Validation score   (f1)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1175.05s of the 1775.23s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t0.5263\t = Validation score   (f1)\n",
      "\t3.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1169.36s of the 1769.54s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=1.52%)\n",
      "\t0.9756\t = Validation score   (f1)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1766.26s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 1.0}\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1766.09s of the 1766.08s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.38%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1763.52s of the 1763.51s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.37%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1760.99s of the 1760.98s of remaining time.\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1760.49s of the 1760.48s of remaining time.\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1760.18s of the 1760.17s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=8.01%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t14.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 1744.25s of the 1744.24s of remaining time.\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 1743.91s of the 1743.90s of remaining time.\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1743.61s of the 1743.60s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1739.86s of the 1739.85s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.81%)\n",
      "\t0.988\t = Validation score   (f1)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1737.26s of the 1737.25s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t3.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1732.25s of the 1732.24s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=1, gpus=0, memory=1.69%)\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1729.36s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini_BAG_L2': 1.0}\n",
      "\t1.0\t = Validation score   (f1)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 70.79s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 20196.7 rows/s (160 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 1.0000\n",
      "\tBest Threshold: 0.500\t| val: 1.0000\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/lpossner/Projects/databot/AutogluonModels/ag-20251030_085810\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train an AutoGluon TabularPredictor model\n",
    "# label:        column to predict (\"test_80\")\n",
    "# eval_metric:  use F1 score for evaluation (suitable for imbalanced OK/NOK data)\n",
    "# train_data:   training dataset without the vehicle identifier\n",
    "# presets:      predefined training configuration balancing speed and quality\n",
    "# num_bag_folds: enables bagging for better generalization (cross-validation based)\n",
    "# num_stack_levels: enables stacking of models for improved accuracy\n",
    "# time_limit:   maximum training time in seconds (here: 30 minutes)\n",
    "predictor = (\n",
    "    TabularPredictor(label=label, eval_metric=\"f1\")\n",
    "    .fit(\n",
    "        train_data=train.drop(columns=[\"igef\"]),\n",
    "        presets=\"medium_quality_faster_train\",\n",
    "        num_bag_folds=5,\n",
    "        num_stack_levels=1,\n",
    "        time_limit=1800\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8751dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 80 features using 200 rows with 5 shuffle sets...\n",
      "\t6.54s\t= Expected runtime (1.31s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model  score_test  score_val eval_metric  \\\n",
      "0           CatBoost_BAG_L1    1.000000   1.000000          f1   \n",
      "1       WeightedEnsemble_L2    1.000000   1.000000          f1   \n",
      "2    NeuralNetFastAI_BAG_L1    1.000000   1.000000          f1   \n",
      "3           LightGBM_BAG_L2    1.000000   1.000000          f1   \n",
      "4      LightGBMLarge_BAG_L2    1.000000   1.000000          f1   \n",
      "5           CatBoost_BAG_L2    1.000000   1.000000          f1   \n",
      "6            XGBoost_BAG_L2    1.000000   0.987952          f1   \n",
      "7     NeuralNetTorch_BAG_L2    1.000000   1.000000          f1   \n",
      "8    NeuralNetFastAI_BAG_L2    1.000000   1.000000          f1   \n",
      "9   RandomForestGini_BAG_L2    1.000000   1.000000          f1   \n",
      "10    ExtraTreesEntr_BAG_L2    1.000000   1.000000          f1   \n",
      "11    ExtraTreesGini_BAG_L2    1.000000   1.000000          f1   \n",
      "12  RandomForestEntr_BAG_L2    1.000000   1.000000          f1   \n",
      "13      WeightedEnsemble_L3    1.000000   1.000000          f1   \n",
      "14     LightGBMLarge_BAG_L1    0.750000   0.975610          f1   \n",
      "15  RandomForestGini_BAG_L1    0.750000   0.923077          f1   \n",
      "16  RandomForestEntr_BAG_L1    0.750000   0.962963          f1   \n",
      "17    ExtraTreesEntr_BAG_L1    0.750000   0.975610          f1   \n",
      "18    ExtraTreesGini_BAG_L1    0.750000   0.962963          f1   \n",
      "19        LightGBMXT_BAG_L2    0.750000   1.000000          f1   \n",
      "20           XGBoost_BAG_L1    0.461538   0.645161          f1   \n",
      "21    NeuralNetTorch_BAG_L1    0.181818   0.526316          f1   \n",
      "22          LightGBM_BAG_L1    0.000000   0.000000          f1   \n",
      "23        LightGBMXT_BAG_L1    0.000000   0.000000          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n",
      "0         0.042618       0.007676   6.743668                 0.042618   \n",
      "1         0.043425       0.008906   6.910707                 0.000807   \n",
      "2         0.331609       0.017968   2.347336                 0.331609   \n",
      "3         0.491189       0.150132  16.650625                 0.003294   \n",
      "4         0.491657       0.150516  16.665669                 0.003762   \n",
      "5         0.499960       0.157869  30.089494                 0.012065   \n",
      "6         0.504356       0.153466  16.431062                 0.016461   \n",
      "7         0.508025       0.168662  18.944407                 0.020130   \n",
      "8         0.512971       0.164083  18.058537                 0.025076   \n",
      "9         0.516567       0.193321  16.121061                 0.028672   \n",
      "10        0.516863       0.194001  15.924120                 0.028968   \n",
      "11        0.517278       0.191684  15.967549                 0.029383   \n",
      "12        0.517707       0.192845  15.934100                 0.029811   \n",
      "13        0.517853       0.192864  16.094442                 0.000575   \n",
      "14        0.008873       0.008070   1.237638                 0.008873   \n",
      "15        0.031088       0.049473   0.521922                 0.031088   \n",
      "16        0.032112       0.047433   0.241472                 0.032112   \n",
      "17        0.032151       0.048447   0.264059                 0.032151   \n",
      "18        0.033456       0.048817   0.299012                 0.033456   \n",
      "19        0.492149       0.151230  16.560963                 0.004254   \n",
      "20        0.028828       0.008055   0.971597                 0.028828   \n",
      "21        0.011704       0.008375   3.878334                 0.011704   \n",
      "22        0.003374       0.002868   0.865402                 0.003374   \n",
      "23        0.139409       0.002392   0.645931                 0.139409   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 0.007676           6.743668            1       True   \n",
      "1                 0.001230           0.167039            2       True   \n",
      "2                 0.017968           2.347336            1       True   \n",
      "3                 0.004108           0.966521            2       True   \n",
      "4                 0.004491           0.981565            2       True   \n",
      "5                 0.011844          14.405390            2       True   \n",
      "6                 0.007441           0.746958            2       True   \n",
      "7                 0.022638           3.260303            2       True   \n",
      "8                 0.018058           2.374433            2       True   \n",
      "9                 0.047296           0.436957            2       True   \n",
      "10                0.047976           0.240016            2       True   \n",
      "11                0.045659           0.283445            2       True   \n",
      "12                0.046820           0.249996            2       True   \n",
      "13                0.001181           0.126893            3       True   \n",
      "14                0.008070           1.237638            1       True   \n",
      "15                0.049473           0.521922            1       True   \n",
      "16                0.047433           0.241472            1       True   \n",
      "17                0.048447           0.264059            1       True   \n",
      "18                0.048817           0.299012            1       True   \n",
      "19                0.005206           0.876859            2       True   \n",
      "20                0.008055           0.971597            1       True   \n",
      "21                0.008375           3.878334            1       True   \n",
      "22                0.002868           0.865402            1       True   \n",
      "23                0.002392           0.645931            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0           5  \n",
      "1          12  \n",
      "2           8  \n",
      "3          14  \n",
      "4          23  \n",
      "5          17  \n",
      "6          21  \n",
      "7          22  \n",
      "8          20  \n",
      "9          15  \n",
      "10         19  \n",
      "11         18  \n",
      "12         16  \n",
      "13         24  \n",
      "14         11  \n",
      "15          3  \n",
      "16          4  \n",
      "17          7  \n",
      "18          6  \n",
      "19         13  \n",
      "20          9  \n",
      "21         10  \n",
      "22          2  \n",
      "23          1  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "[[190   0]\n",
      " [  0  10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.55s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         importance    stddev   p_value  n  p99_high   p99_low\n",
      "test_2     0.360000  0.036515  0.000013  5  0.435185  0.284815\n",
      "test_1     0.283072  0.091498  0.001146  5  0.471467  0.094677\n",
      "test_4     0.214436  0.035653  0.000088  5  0.287846  0.141026\n",
      "test_3     0.098195  0.045124  0.004122  5  0.191107  0.005284\n",
      "test_61    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_60    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_6     0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_59    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_58    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_57    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_56    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_0     0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_55    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_62    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_53    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_52    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_51    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_50    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_5     0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "test_49    0.000000  0.000000  0.500000  5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on the held-out test set\n",
    "\n",
    "# 1) Generate a leaderboard of all trained models\n",
    "#    Shows model names, validation scores, training times, and performance on the test set\n",
    "leaderboard = predictor.leaderboard(test.drop(columns=[\"igef\"]), silent=True)\n",
    "print(leaderboard)\n",
    "\n",
    "# 2) Generate predictions for the test set\n",
    "#    Drop \"igef\" since it is only an identifier and not a predictive feature\n",
    "y_true = test[label]\n",
    "y_pred = predictor.predict(test.drop(columns=[\"igef\"]))\n",
    "\n",
    "# 3) Display classification metrics\n",
    "#    Shows precision, recall, F1-score, and support for each class (OK/NOK)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 4) Display confusion matrix\n",
    "#    Rows = true labels, Columns = predicted labels\n",
    "#    Helps visualize false positives/negatives\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# 5) Compute and display feature importance\n",
    "#    Quantifies how much each test (feature) contributes to predicting test_80\n",
    "features_importances = predictor.feature_importance(test.drop(columns=[\"igef\"]))\n",
    "print(features_importances.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
